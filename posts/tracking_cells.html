<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
<head>
  <title>J. Mabon</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script> -->
  <!-- <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script> -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'> -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
</head>
<body>

  <!-- <div id="navbar"></div> -->
  <!-- <script src="/navbar.js"></script> -->

  <div class="container">
    <div class="row">
        <h1>
            Mycobacteria tracking on time-lapse imaging
        </h1>
        <h2>
            My internship at EMBL-EBI in Cambridge with Virginie Uhlmann's group
        </h2>
    </div>

    <div class="row">
      <div class="col-md-8" style="height: 100vh;">
        <div style="margin-top:3%; text-align:justify;"> 

            <p>During summer 2019 I had the pleasure to do an internship with <a href="https://www.ebi.ac.uk/research/uhlmann">Virginie Uhlmann ‘s team</a> at the <a href="https://www.ebi.ac.uk">European Bioinformatics Institute (EBI)</a> . The team aims at developing tools that blend mathematical models and image processing algorithms to quantitatively characterize the content of bioimages.</p>

<p>In this post I will discuss the methods used during the course of this internship on <em>multiple cell tracking in time lapse microscopy images</em>. Most of the methods described in this post were adapted from the cited publications to our case and implemented by me in <strong>Python</strong> and <strong>Tensorflow</strong>.</p>

<h1 id="goal">Goal</h1>

<p>My goal during this 6 months internship was to further develop techniques to track individual <em>Mycobacteria smegmatis</em> cells in sequences of time-lapse microscopy images.
Since these images are challenging because of <strong>division events</strong>, <strong>compact cell colonies</strong> and little prior on individual <strong>cell shapes</strong>, classical segment-then-track methods are ineffective. We relied on a <strong>graphical model</strong> solution to solve the <strong>tracking</strong> and <strong>segmentation</strong> problem jointly at once on the whole sequence. We also need to identify <strong>division events</strong> and thus build a <em>Mycobacteria smegmatis</em> genealogical tree of some sorts.
To build the graphical model, cell <strong>candidates</strong> must be identified in each individual image. To do so, we explored the use of several convolutional neural network models from U-net to discriminative losses for instance segmentation.</p>

<h2 id="mycobacteria-smegmatis"><em>Mycobacteria smegmatis</em></h2>
<p>We were working on time lapse images of <em>Mycobacteria smegmatis</em> that is a non pathogenic bacterial species that strongly resembles the <em>Mycobacterium tuberculosis</em>, making it a great tool to study pathogens in a safe environment. Several works <a class="citation" href="#Santi2013">(Santi et al., 2013; Santi &amp; McKinney, 2015)</a> study the replication mechanism of these cells and how it is linked to antibiotics resistance</p>

<figure>
	<center>
  	<img src="/img/posts/tracking_cells/mycobacteria.png" alt="Mycobacteria smegmatis" style="width:75%" />
  	<figcaption>Mycobacteria smegmatis in phase microscopy overlayed with fluorescence data <a class="citation" href="#ginda2017studies">(Ginda et al., 2017)</a></figcaption>
  	</center>
</figure>

<h2 id="the-data">The data</h2>
<p>The data (courtesy of LMIC, EPFL, Lausanne, Switzerland) consist of several time-lapse microscopy videos showing the growth of M. smegmatis colonies. Images are composed of a phase contrast and a fluorescence channel reporting cellular division. In approximately the first 3/4 of the frames of each videos, the medial axis of the bacterias have been annotated with a <a href="http://bigwww.epfl.ch/teaching/projects/abstracts/mariani/">custom software</a>.</p>

<h2 id="ambiguity-in-single-frame-segmentation">Ambiguity in single frame segmentation</h2>
<p>The main issue with this data is that the detection relies mainly on the temporal data since on a given frame it is not obvious to the human eye where the boundaries of the cells are since cells are often cluttered.</p>
<figure>
	<center>
  	<img src="/img/posts/tracking_cells/detection.png" alt="Detection ambiguity" style="width:100%" />
  	<figcaption>Example of different detection solutions for a single image (the input image in this figure is a substitute of the raw data)</figcaption>
  	</center>
</figure>
<p>Here a simple input frame with fews cells can lead to multiple hypothesis on the number and placement of cells in the frame. This shows how there is ambiguity on individual cells detection even in a few cell setup. Later in the time-lapse there can be thousands of cells at once. Therefore, because of the in-frame ambiguity, we cannot simply segment each frame and then track between frames.</p>

<p>From that example one can intuit that some solutions are more likely than others, also by using the data from the previous and next frame we may have more insight on the most probable solution. For that matter we used a graphical model to make use of all of the data available in order to solve the detection and tracking.</p>

<hr />

<h1 id="graphical-models-for-joint-segmentation-and-tracking">Graphical models for joint Segmentation and tracking</h1>

<p>Schiegg et al. propose a model based on factor graphs to represent the multiple detection hypothesis and jointly tracking and segmenting on whole videos at once <a class="citation" href="#schiegg2014graphical">(Schiegg et al., 2014)</a>. To put it in a nutshell, given that we provide a set of <strong>detection candidates</strong> and <strong>transition candidates</strong> , along with <strong>associated probabilities</strong> , this method uses a graph representation to compute the most likely candidates for detections and transitions within some <strong>constraints</strong> .</p>

<ul>
  <li><strong>detection candidates</strong> are potential cells (for instance the region labeled 1 in the figure, or the union of 2 and 3 labeled 23)</li>
  <li><strong>transition candidates</strong> represent how these candidates would transition from one frame to the next. Here one could expect 12 (at frame t)to transition into 4 (at frame t+1)</li>
  <li><strong>constraints</strong> are what makes the solution physically sound, no cell candidates can overlap (cell candidate 12 could not co-exist with 23), cells should not appear and disappear in the middle of the video etc…</li>
</ul>

<p>For each of the <strong>detection candidates</strong> and <strong>transition candidates</strong> we can provide an associated <strong>probability</strong>, this is provided by a simple classifier that uses as input some cell data (shape, length etc..) to infer its likelihood of being a cell or transition regardless of context. For instance a one pixel cell would have a very low probability and a transition implying that a cell would travel from one end of the image to the other in a single frame is also rated with a low probability.</p>

<figure>
	<center>
  	<img src="/img/posts/tracking_cells/graphical_model_complete.png" alt="A graphical model" style="width:100%" />
  	<figcaption>Sample graphical model for only two frames, each <b>detection candidate</b> is represented by a green node. Yellow nodes correspond to <b>transition candidates</b> between frames. <b>Constraints</b> are represented by the square nodes</figcaption>
  	</center>
</figure>

<p>To explain the technical functioning of a graphical model would take another presentation, for this time we will focus on how to generate these segmentation proposals. Also this part of the graphical inference model had already been implemented by C. Haubold <a class="citation" href="#haubold2017scalable">(Haubold, 2017)</a>.</p>

<h2 id="graphical-model-model-limitation">Graphical model model limitation</h2>
<p>From the previous example we can expect the graph to grow exponentially as the number cell proposal augments. Thus it is important to generate a set of segmentation proposals that is wide enough to contain the ground truth but small enough not to make the complexity explode.</p>

<h1 id="previous-approach">Previous approach</h1>
<p><em>BactImAS</em> <a class="citation" href="#mekterovic2014bactimas">(Mekterović et al., 2014)</a> offers a semi-automated solution to the tracking problem where the user must manually define initial cells, division events, and maybe correct the model’s prediction in between. To detect cells <em>BactImAS</em> relies on edge detection, thresholds and skeletonization.
In her PhD thesis <a class="citation" href="#uhlmann2017landmark">(Uhlmann, 2017)</a>, Virginie Uhlman introduces splines to further automate the process. From the detected cell tips a set of shortest viable path in between are generated as cell proposals. The issue of the exploding graph complexity remains because of the high number of proposals.</p>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/uhlmann-prev-appr.jpg" alt="Uhlmann previous approach" style="width:100%" />
    <figcaption>V. Uhlmann's approach. 1 : identify cell tips with pixel classifier, 2 and 3 : model all possible tip to tip links as a shortest path relying on splines <a class="citation" href="#uhlmann2017landmark">(Uhlmann, 2017)</a></figcaption>
    </center>
</figure>

<hr />

<h1 id="exploring-candidates-generation-through-deep-learning">Exploring candidates generation through deep learning</h1>

<p>The aim of my internship project was to explore the generation of cell candidates using deep learning methods. The idea being that a number of cell proposal could be ruled out by their improbable shape, thus adding a learning component (opposed to doing only image processing) to the process could improve the proposals.</p>

<h2 id="first-approach--u-net-for-pixel-class-prediction">First approach : U-net for pixel class prediction</h2>

<p>Graphical models provide a way to solve for the tracking and segmentation problem simultaneously on the entire video. However, it relies on cell candidates generated for each frame. Since we want the true segmentation to be within the set of candidates, we perform <strong>over-segmentation</strong> (i.e. we segment too much) to ensure that we obtain more false positives that false negatives (for that we can rule out false positives with the graphical model).
Deep neural networks can be used to generate such segmentation proposals in a robust and generalisable way.
For instance, <a class="citation" href="#Falk2019">(Falk et al., 2019)</a> propose a deep learning method for detecting instances of cells in images relying on pixel-wise classification. For that matter we used a U-Net architecture.</p>

<h3 id="what-is-a-u-net-">What is a U-net ?</h3>
<p><strong>U-net</strong> is a convolutional neural network first introduced by <a class="citation" href="#RonnebergerFB15">(Ronneberger et al., 2015)</a> for biomedical image segmentation. It aims at providing fine pixel classification with low and high scale awareness of the neighboring pixels. The network is composed of a contracting path that decreases the image size through <strong>convolution</strong> and <strong>pooling</strong> while increasing the number of channels, and an expansive path that increases the image size through <strong>convolution</strong> and <strong>up-sampling</strong> while decreasing the number of channels. The output image scale  is therefore the same as the input. <strong>Skip paths</strong> are also added to link layers  within levels, so that the information capturing fine details can be forwarded to the end of the network without traversing the contracting path.</p>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/u-net-architecture.png" alt="Unet Architecture" style="width:100%" />
    <figcaption>Structure of a basic U-net <a class="citation" href="#RonnebergerFB15">(Ronneberger et al., 2015)</a>. The <b>skip paths</b> are shown in gray</figcaption>
    </center>
</figure>

<h3 id="weighted-soft-max-cross-entropy-loss">Weighted soft-max cross-entropy loss</h3>

<p>We use a Cross entropy loss with soft-max as it is commonly used for classification <a class="citation" href="#Bishop:2006:PRM:1162264">(Bishop, 2006)</a> and can be written as</p>

\[l(I) := - \sum_{x\in I}w(x) \log
\frac{
\exp \left(    \hat{y}_{y(x)}(x)  \right)
}{
\sum_{k=0}^K\exp \left(    \hat{y}_{k}(x)  \right)
},\]

<p>with \(x\) a pixel in the image domain \(I\), \(\hat{y}_k:I\rightarrow\mathbb{R}\) the predicted score for class \(k\), \(K\) the number of classes, and \(y:I\rightarrow\{0,\dots,K\}\) the ground truth segmentation. Thus, \(\hat{y}_{y(x)}(x)\) corresponds to the predicted score for ground-truth class \(y(x)\) at position \(x\). The \(w(x)\) is a per-pixel weighting term used to handle class imbalance and instance separation, and is defined as</p>

\[w:=w_\mathrm{bal} +w_\mathrm{sep}\]

<h4 id="learning-instance-segmentation">Learning instance segmentation</h4>

<p>To learn how to segment instances, labels of different instances must be separated by at least one pixel of background, ensuring that each instance is one single connected component. In order for this gap to be predicted correctly by the network, a weighting term \(w_\mathrm{sep}\) is applied on the loss to further penalize errors in boundary areas as</p>

\[w_\mathrm{sep}(x) := \exp \left( -\frac{(d_1(x)+d_2(x)}{2\sigma^2}\right),\]

<p>with \(d_1\) and \(d_2\) the distances to the two nearest instances. This weight therefore increases at locations where two cells are close together, enforcing a separation. The next figure shows an illustration of  \(w_\mathrm{sep}(x)\) over an image.</p>

<h4 id="pixel-class-prediction">Pixel class prediction</h4>

<p>Our first objective is to distinguish cells from background, thus we use a background class and a cell class. However individual cells need to be spatially separated by a gap in order to distinguish different entities, hence we introduce an <strong>inner cell</strong> and <strong>outer cell</strong> class in order to have separate cell cores between and also not to confuse the classifier by labeling the outer cells as background. Since the cell tips will be a useful information to separate individual cells afterwards we also introduce a <strong>cell tip</strong> class.</p>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/unet-train-weighted.png" alt="Unet training" style="width:100%" />
    <figcaption>Training the U-net with our data.</figcaption>
    </center>
</figure>

<p>Once the U-net is trained, the network outputs probability maps for each class.</p>

<h3 id="candidates-generation">Candidates generation</h3>

<p>From output of the network we need to produce a set of cell candidates. We devised a method of <em>over-segmentation</em> to get a set of cell parts that could be assembled to make cell candidates. Basically, from the separated <strong>inner cells</strong>, we run a watershed algorithm to propagate labels on the cells. We get the results shown below on a sample image. From the over-segmentation output we can build a set of cell proposals. We build proposals from the cell segments by proposing every segment and every union of touching segments (within a set limit amount, so that cells aren’t too big).</p>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/candidates.png" alt="Candidates generation" style="width:100%" />
    <figcaption>Building candidates from the predicted pixel classes. Left : the predicted pixel classes. Center : cell segments built from the pixel classification. Right : set of cell candidates built from the available cell segments.</figcaption>
    </center>
</figure>

<h3 id="results">Results</h3>
<p>This pixel wise classification approach provided decent results, however the candidates generation part relies heavily on <strong>manually chosen hyper-parameters</strong>. Also this methods tends to generate quite a lot of very small cell proposals. These numerous false positive make the graphical model complexity explode. Any attempt at choosing a set of hyper-parameters to reduce the number of false positives dramatically increase the number of false negatives, meaning that the graphical model cannot be set up.</p>

<hr />

<h2 id="instance-segmentation-with-pixel-embedding">Instance segmentation with pixel embedding</h2>
<p>In the first approach the instance segmentation (differentiating one cells from its neighbor) is done by the proxy of <strong>per pixel classification</strong>. What if we could directly optimize for <strong>instance segmentation</strong> within the convolutional network and thus minimize the handcrafting of the cell proposal method ?</p>

<p>We thus focus on a framework in which the instance segmentation problem becomes a <strong>pixel clustering problem</strong> in a new feature space <a class="citation" href="#deBrabandere2017semantic">(De Brabandere et al., 2017)</a>. This approach allows predicting instance segmentation with no prior on the number of elements in the image. In constrast to other popular instance segmentation methods like Mask R-CNN <a class="citation" href="#he2017mask">(He et al., 2017)</a>, it does not rely on region proposal followed by classification. Instead of doing pixel classification with a softmax loss, which would limit the number of instances, we can detect any number of instances in an image can be captured.
Indeed, when predicting instances as one class for each instance, the number of instances is limited by the size of the output vector, i.e. the number of classes. Also
<a class="citation" href="#payer2018instance">(Payer et al., 2018)</a> extend this idea by considering a tracking component to the problem, adding recurrent components in the network and using a new similarity measure in the feature space.</p>

<p>In this framework each pixel is attributed a <strong>N dimensional vector</strong>, so that every pixel from a same cell has a “similar” vector and pixels from different cells have “non-similar” vectors (the similarity measure may depend on the method used). If N=3 you can consider these vectors to be RGB colors, the objective being so that each individual cell is colored uniformly, but different cells have different colors. Through the different figures we project our N dimensional vectors to a 3D space with a PCA so that we can display these vectors as colors. Using the first two dimensions of the PCA we can plot these pixels as points within this 2D space to better visualize the clustering.</p>

<h3 id="semantic-instancesegmentation-with-a-discriminative-loss-function">Semantic InstanceSegmentation with a Discriminative Loss Function</h3>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/SIS-DLF-1.jpg" style="width:100%" />
    <img src="/img/posts/tracking_cells/SIS-DLF-2.jpg" style="width:100%" />
    <figcaption>Top left: the input image. Top right: pixel embeddings projected in a 2D space each car is a cluster. Bottom right: pixel embeddings interpreted as colors, each individual car is colored/projected differently. Bottom right: predicted instances after clustering of the pixel embeddings <a class="citation" href="#deBrabandere2017semantic">(De Brabandere et al., 2017)</a></figcaption>
    </center>
</figure>

<h4 id="discriminative-loss">discriminative loss</h4>
<p>To learn the clustering, we rely on a discriminative loss composed of three key parts <a class="citation" href="#deBrabandere2017semantic">(De Brabandere et al., 2017)</a>. During learning we use as input the image along with ground truth instance labels.</p>

<ul>
  <li><strong>Variance term</strong>: is an intra-cluster pull force moving embeddings toward the mean of each label cluster. A margin is set for the variance, which corresponds to the inner circle in the next figure. This margin defines how tightly packed clusters should be.</li>
  <li><strong>Distance term</strong>: an inter-cluster push force drawing the mean of embeddings of different instances further
apart from each other. A margin is set for the distance, corresponding to the outer
circle in the next figure. This margin defines how far from each other clusters should be.</li>
  <li><strong>Regularization term</strong>: it is a pull force drawing clusters closer to the origin of the embedding space to avoid values blowing up.</li>
</ul>

<p>These three part as summed as the loss function and minimized during the learning phase.</p>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/SIS-DLF-3.jpg" style="width:100%" />
    <figcaption> <a class="citation" href="#deBrabandere2017semantic">(De Brabandere et al., 2017)</a></figcaption>
    </center>
</figure>

<h3 id="clustering">Clustering</h3>
<p>For that matter, we used the agglomerative clustering available in sklearn
<a class="citation" href="#scikit-learn">(Pedregosa et al., 2011)</a>. This method starts by initializing each pixel as a cluster. Clusters are
then merged together in a tree-like manner. The merging stops when a distance threshold is met,
that is when the distance between all clusters is more than the threshold. This is relatable to the
design of the discriminative loss since it enforces a minimal distance between different instances. Also this method does not rely on a prior knowledge of the number of clusters.</p>

<h3 id="results-1">Results</h3>

<p>Here is the results on a few sample frames. From the second more cluttered frame we observe that this method scales decently to a high number of cells. One improvement might be to add a filter on the smallest size a cluster can be to avoid 1 pixel clusters being proposed as cells.</p>

<figure>
  <center>
    <img src="/img/posts/tracking_cells/clustering_18_35.png" style="width:100%" />
    <figcaption> Result on an early frame</figcaption>
    </center>
</figure>

<hr />

<figure>
  <center>
    <img src="/img/posts/tracking_cells/clustering_18_120.png" style="width:100%" />
    <figcaption> Result on a more cluttered frame</figcaption>
    </center>
</figure>

<h2 id="other-approaches-and-perspectives">Other approaches and perspectives</h2>
<p>We tried implementing a Cosine embedding loss <a class="citation" href="#payer2018instance">(Payer et al., 2018)</a> as a replacement for the discriminative loss function. This yielded similar results but is not as straightforward to cluster as instead of using any N dimensional vector, the pixels are embedded in a N dimensional sphere. Meaning that the similarity function for clustering is not longer a simple euclidean distance but rather a cosine similarity. Also <a class="citation" href="#payer2018instance">(Payer et al., 2018)</a> proposes to compute the loss over several frame and ensure that cells have a consistent embedding cluster through time, thus learning the <strong>tracking</strong> part along the <strong>instance segmentation</strong> part. Due to time constraints I did not manage to properly test this promising method.</p>

<p>Another idea would be to expand this embedding time consistency to cell divisions, for instance we could devise a loss so that, once projected in a set of specified dimension parent cells would be similar, but remains separate in the other dimensions. If functioning properly this method would negate the need of the graphical model as it solves for <strong>tracking</strong>, <strong>instance segmentation</strong> and <strong>cell divisions</strong> at once.</p>

<hr />

<h1 id="conclusion">Conclusion</h1>
<p>From the different convolutional network methods I used I managed to produce strong sets of cell proposals. However due to time constraints and technical limitation of the graphical model, I did not manage to properly use these cell proposals to train the latter. Meanwhile this project gave me a great insight on the functioning and building on convolutional networks.</p>

<p>Throughout this project, the graphical model was a motivation for our model design choices. A U-net architecture provides an instance segmentation that is not satisfactory as it has extensive post-processing that is specific to the dataset. As we aim for generalizable solution we also explored instance segmentation oriented networks and losses (e.g. discriminative or cosine embedding loss). These offer a more straightforward optimization process to obtain cell candidates for the graphical model. Moreover, the clustering process offers better control on the roughness or finesse of the segmentation candidates.</p>

<p>Unfortunately, the ground truth matching part which is necessary to train the graphical model, proved to be a roadblock when using our first instance predictions obtained using U-net on a pixel classification task. We sadly did not have time to test ground piping our discriminative network results into the graphical model.</p>

<hr />

<h1 id="acknowledgments">Acknowledgments</h1>

<p>To the French Embassy in London for funding this internship, to EMBL for making it
possible, to José for his technical help,
And mostly, warm and sincere thanks to the whole Uhlmann Lab : Virginie, Soham, Yoann,
Johannes, James and Maria.</p>

<hr />

<h2 id="bonus-making-some-abstract-art-with-it">Bonus: Making some abstract “art” with it</h2>

<figure>
  <center>
  <img src="/img/cells.png" style="width:100%" />
  </center>
</figure>

<p>As i noticed the network trained on the discriminative loss function outputted some pretty colors I tried to feed it a synthetic image over-packed with cells. This is the result, it makes for a great (but maybe too colorful) wallpaper.</p>
<figure>
  <center>
    <img src="/img/posts/tracking_cells/super_spagettos.png" style="width:100%" />
    <figcaption> Left: Input spaghettis. Right: output colors</figcaption>
    </center>
</figure>

<hr />

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="Santi2013">Santi, I., Dhar, N., Bousbaine, D., Wakamoto, Y., &amp; McKinney, J. D. (2013). Single-cell dynamics of the chromosome replication and cell division cycles in mycobacteria. <i>Nature Communications</i>, <i>4</i>, 2470 EP  -. https://doi.org/10.1038/ncomms3470</span></li>
<li><span id="Santie01999-14">Santi, I., &amp; McKinney, J. D. (2015). Chromosome Organization and Replisome Dynamics in Mycobacterium smegmatis. <i>MBio</i>, <i>6</i>(1). https://doi.org/10.1128/mBio.01999-14</span></li>
<li><span id="ginda2017studies">Ginda, K., Santi, I., Bousbaine, D., Zakrzewska-Czerwińska, J., Jakimowicz, D., &amp; McKinney, J. (2017). The studies of ParA and ParB dynamics reveal asymmetry of chromosome segregation in mycobacteria. <i>Molecular Microbiology</i>, <i>105</i>(3), 453–468.</span></li>
<li><span id="schiegg2014graphical">Schiegg, M., Hanslovsky, P., Haubold, C., Koethe, U., Hufnagel, L., &amp; Hamprecht, F. A. (2014). Graphical model for joint segmentation and tracking of multiple dividing cells. <i>Bioinformatics</i>, <i>31</i>(6), 948–956.</span></li>
<li><span id="haubold2017scalable">Haubold, C. (2017). <i>Scalable Inference for Multi-target Tracking of Proliferating Cells</i> [PhD thesis].</span></li>
<li><span id="mekterovic2014bactimas">Mekterović, I., Mekterović, D., &amp; others. (2014). BactImAS: a platform for processing and analysis of bacterial time-lapse microscopy movies. <i>BMC Bioinformatics</i>, <i>15</i>(1), 251.</span></li>
<li><span id="uhlmann2017landmark">Uhlmann, V. S. (2017). <i>Landmark active contours for bioimage analysis: A tale of points and curves</i> [PhD thesis]. Ecole Polytechnique Fédérale de Lausanne.</span></li>
<li><span id="Falk2019">Falk, T., Mai, D., Bensch, R., Çiçek, Ö., Abdulkadir, A., Marrakchi, Y., Böhm, A., Deubner, J., Jäckel, Z., Seiwald, K., Dovzhenko, A., Tietz, O., Dal Bosco, C., Walsh, S., Saltukoglu, D., Tay, T. L., Prinz, M., Palme, K., Simons, M., … Ronneberger, O. (2019). U-Net: deep learning for cell counting, detection, and morphometry. <i>Nature Methods</i>, <i>16</i>(1), 67–70. https://doi.org/10.1038/s41592-018-0261-2</span></li>
<li><span id="RonnebergerFB15">Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. <i>CoRR</i>, <i>abs/1505.04597</i>. http://arxiv.org/abs/1505.04597</span></li>
<li><span id="Bishop:2006:PRM:1162264">Bishop, C. M. (2006). <i>Pattern Recognition and Machine Learning (Information Science and Statistics)</i>. Springer-Verlag.</span></li>
<li><span id="deBrabandere2017semantic">De Brabandere, B., Neven, D., &amp; Van Gool, L. (2017). Semantic instance segmentation with a discriminative loss function. <i>ArXiv Preprint ArXiv:1708.02551</i>.</span></li>
<li><span id="he2017mask">He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2017). Mask r-cnn. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 2961–2969.</span></li>
<li><span id="payer2018instance">Payer, C., Štern, D., Neff, T., Bischof, H., &amp; Urschler, M. (2018). Instance segmentation and tracking with cosine embeddings and recurrent hourglass networks. <i>International Conference on Medical Image Computing and Computer-Assisted Intervention</i>, 3–11.</span></li>
<li><span id="scikit-learn">Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. <i>Journal of Machine Learning Research</i>, <i>12</i>, 2825–2830.</span></li></ol>

        </div>
      </div>
    </div>
  </div>
</body>